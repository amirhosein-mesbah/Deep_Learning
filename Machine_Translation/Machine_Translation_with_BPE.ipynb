{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dofUiuBdYv7v"
      },
      "source": [
        "# import data and libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XODLWxzYyfc"
      },
      "source": [
        "- download data from google drive to kaggle \n",
        "- unzip data\n",
        "- upgrade torch, torchtext, torchvision and nltk libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh7hff_hYkV_"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb8SVCseZ76N",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "! pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgTW4lcWZ76T",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "! gdown 'https://drive.google.com/u/0/uc?id=1AoiQCXqFbqETGILCpgFgozYJvmyGfq_9&export=download'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JelZOirzZ76U",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "! gdown 'https://drive.google.com/u/0/uc?id=1zmdERncg0zcrqpdCzOzth0exYevXxEp2&export=download'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89PBeT43Z76V",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "! unzip './AFEC-merged-all.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2ddFXDfZ76W",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!unzip './Test.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQiiY4orZ76W",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "! pip install -q i "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqQUJQRlbNeJ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bizLi994cx9P"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9bQwRArZTnP"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fELKMR0bY77a"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHsSyaQY5lF"
      },
      "source": [
        "- read data line by line\n",
        "- create a pandas data frame\n",
        "- split data to train (90%) and validation (10%)\n",
        "- save train and validation data frames\n",
        "\n",
        "> with help of this [tutorail](https://youtu.be/DaHAzCaXWYQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:02.386207Z",
          "iopub.status.busy": "2021-06-30T02:50:02.385834Z",
          "iopub.status.idle": "2021-06-30T02:50:02.690917Z",
          "shell.execute_reply": "2021-06-30T02:50:02.690071Z",
          "shell.execute_reply.started": "2021-06-30T02:50:02.386121Z"
        },
        "id": "ZbC2f8kdZ76X",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:02.692601Z",
          "iopub.status.busy": "2021-06-30T02:50:02.692262Z",
          "iopub.status.idle": "2021-06-30T02:50:04.709987Z",
          "shell.execute_reply": "2021-06-30T02:50:04.708898Z",
          "shell.execute_reply.started": "2021-06-30T02:50:02.692575Z"
        },
        "id": "Ep3_U7HaZ76Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "text_fa = open('./AFEC-merged.fa', encoding='utf8').read().splitlines()\n",
        "text_eng =  open('./AFEC-merged.en', encoding='utf8').read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:04.712071Z",
          "iopub.status.busy": "2021-06-30T02:50:04.711730Z",
          "iopub.status.idle": "2021-06-30T02:50:04.716229Z",
          "shell.execute_reply": "2021-06-30T02:50:04.715451Z",
          "shell.execute_reply.started": "2021-06-30T02:50:04.712036Z"
        },
        "id": "NtFrNELGZ76Y",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "persian_english_text = {'English':text_eng, 'Persian':text_fa}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:04.718270Z",
          "iopub.status.busy": "2021-06-30T02:50:04.717714Z",
          "iopub.status.idle": "2021-06-30T02:50:05.161294Z",
          "shell.execute_reply": "2021-06-30T02:50:05.159638Z",
          "shell.execute_reply.started": "2021-06-30T02:50:04.718235Z"
        },
        "id": "RpGvuTpEZ76Z",
        "outputId": "e759dd21-6728-4253-d5e6-bc8e2db437c5",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Persian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>North Waziristan operation kills 50 more milit...</td>\n",
              "      <td>مرگ 50 ستیزه جوی دیگر در عملیات وزیرستان شمالی</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PESHAWAR - The on - goingmilitary operation in...</td>\n",
              "      <td>پیشاور - به گزارش جیو نیوز , عملیات ادامه دار ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Pakistani Air Force conducted airstrikes a...</td>\n",
              "      <td>به گزارش رسانه‌ها , نیروی هوایی پاکستان حملات ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A day earlier , at least 27 militants were rep...</td>\n",
              "      <td>یک روز پیشتر گزارش شده بود که دست کم 27 ستیزه ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Meanwhile , a roadside blast on Bangidar road ...</td>\n",
              "      <td>در ضمن , مسئولان گفتند که یک انفجار کنارجاده ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684412</th>\n",
              "      <td>that is an excellent idea . when is your birth...</td>\n",
              "      <td>این یک فکر عالی است . تولد شما کی است ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684413</th>\n",
              "      <td>fine , then we will go back by train to Hambur...</td>\n",
              "      <td>خوب , بنابراین ما با قطار در سه و سی و سه دقیق...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684414</th>\n",
              "      <td>well , I think , eight o ' clock in the evenin...</td>\n",
              "      <td>خوب , من فکر میکنم , ساعت هشت بعداز ظهر کافی ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684415</th>\n",
              "      <td>there is a train at thirty-three past six o ' ...</td>\n",
              "      <td>یک قطار برای ساعت شش و سی و سه دقیقه وجود دارد...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684416</th>\n",
              "      <td>goodbye .</td>\n",
              "      <td>خداحافظ .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>684417 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  English  \\\n",
              "0       North Waziristan operation kills 50 more milit...   \n",
              "1       PESHAWAR - The on - goingmilitary operation in...   \n",
              "2       The Pakistani Air Force conducted airstrikes a...   \n",
              "3       A day earlier , at least 27 militants were rep...   \n",
              "4       Meanwhile , a roadside blast on Bangidar road ...   \n",
              "...                                                   ...   \n",
              "684412  that is an excellent idea . when is your birth...   \n",
              "684413  fine , then we will go back by train to Hambur...   \n",
              "684414  well , I think , eight o ' clock in the evenin...   \n",
              "684415  there is a train at thirty-three past six o ' ...   \n",
              "684416                                         goodbye .    \n",
              "\n",
              "                                                  Persian  \n",
              "0          مرگ 50 ستیزه جوی دیگر در عملیات وزیرستان شمالی  \n",
              "1       پیشاور - به گزارش جیو نیوز , عملیات ادامه دار ...  \n",
              "2       به گزارش رسانه‌ها , نیروی هوایی پاکستان حملات ...  \n",
              "3       یک روز پیشتر گزارش شده بود که دست کم 27 ستیزه ...  \n",
              "4       در ضمن , مسئولان گفتند که یک انفجار کنارجاده ا...  \n",
              "...                                                   ...  \n",
              "684412           این یک فکر عالی است . تولد شما کی است ?   \n",
              "684413  خوب , بنابراین ما با قطار در سه و سی و سه دقیق...  \n",
              "684414  خوب , من فکر میکنم , ساعت هشت بعداز ظهر کافی ا...  \n",
              "684415  یک قطار برای ساعت شش و سی و سه دقیقه وجود دارد...  \n",
              "684416                                         خداحافظ .   \n",
              "\n",
              "[684417 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df =pd.DataFrame(persian_english_text, columns=['English', 'Persian'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:05.162785Z",
          "iopub.status.busy": "2021-06-30T02:50:05.162439Z",
          "iopub.status.idle": "2021-06-30T02:50:05.411263Z",
          "shell.execute_reply": "2021-06-30T02:50:05.410422Z",
          "shell.execute_reply.started": "2021-06-30T02:50:05.162750Z"
        },
        "id": "WTodDhx-Z76a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train, validation = train_test_split(df, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:05.413014Z",
          "iopub.status.busy": "2021-06-30T02:50:05.412513Z",
          "iopub.status.idle": "2021-06-30T02:50:13.143801Z",
          "shell.execute_reply": "2021-06-30T02:50:13.142878Z",
          "shell.execute_reply.started": "2021-06-30T02:50:05.412976Z"
        },
        "id": "v0tQOqdaZ76a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train.to_csv('train.csv', index=False)\n",
        "validation.to_csv('validation.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC-jXdRUZwVX"
      },
      "source": [
        "# Create dataset and dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrCfU0KEZI6G"
      },
      "source": [
        "- create persian and english tokenizer functions\n",
        "- create 2 Field for persian and english\n",
        "- create training and validation data from train and validation data frames\n",
        "- build vocabulary for 2 langages with frequency threshold of 5 and maximum size of 20000\n",
        "- craete train and validation data loaders with batch size of 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> with help of this [tutorail](https://youtu.be/DaHAzCaXWYQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:19.477061Z",
          "iopub.status.busy": "2021-06-30T02:50:19.476702Z",
          "iopub.status.idle": "2021-06-30T02:50:19.948685Z",
          "shell.execute_reply": "2021-06-30T02:50:19.947866Z",
          "shell.execute_reply.started": "2021-06-30T02:50:19.477024Z"
        },
        "id": "MaRdiDX7Z76b",
        "outputId": "5f69376b-8008-4e51-bf19-56478dbb8941",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm_sqlR6Z4o5"
      },
      "source": [
        "## craete tokenizer functions by pyonmttok\n",
        "-  train a BPE tokenizer on each language sentences\n",
        "- use trained BPE tokenizer as a tokenization Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiKlAwbDa692"
      },
      "source": [
        "### English BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:19.950467Z",
          "iopub.status.busy": "2021-06-30T02:50:19.950099Z",
          "iopub.status.idle": "2021-06-30T02:50:19.960267Z",
          "shell.execute_reply": "2021-06-30T02:50:19.959441Z",
          "shell.execute_reply.started": "2021-06-30T02:50:19.950431Z"
        },
        "id": "pp-AE2s1Z76b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pyonmttok\n",
        "# BPE is trained and applied on the tokenization output before joiner (or spacer) annotations.\n",
        "tokenizer_english = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, segment_numbers=True)\n",
        "learner = pyonmttok.BPELearner(tokenizer=tokenizer_english, symbols=32000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:19.962033Z",
          "iopub.status.busy": "2021-06-30T02:50:19.961624Z",
          "iopub.status.idle": "2021-06-30T02:50:28.558133Z",
          "shell.execute_reply": "2021-06-30T02:50:28.557196Z",
          "shell.execute_reply.started": "2021-06-30T02:50:19.961996Z"
        },
        "id": "zZfUANjtZ76c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in range(len(text_eng)):\n",
        "  learner.ingest(text_eng[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:28.559707Z",
          "iopub.status.busy": "2021-06-30T02:50:28.559370Z",
          "iopub.status.idle": "2021-06-30T02:50:36.699314Z",
          "shell.execute_reply": "2021-06-30T02:50:36.698439Z",
          "shell.execute_reply.started": "2021-06-30T02:50:28.559674Z"
        },
        "id": "aIPkp2OpZ76c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer_english = learner.learn(\"model-32k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_QmxkX0a9cn"
      },
      "source": [
        "### Persian BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:36.708754Z",
          "iopub.status.busy": "2021-06-30T02:50:36.708380Z",
          "iopub.status.idle": "2021-06-30T02:50:36.718290Z",
          "shell.execute_reply": "2021-06-30T02:50:36.717451Z",
          "shell.execute_reply.started": "2021-06-30T02:50:36.708717Z"
        },
        "id": "NXpjTLJLZ76d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# BPE is trained and applied on the tokenization output before joiner (or spacer) annotations.\n",
        "tokenizer_persian = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, segment_numbers=True)\n",
        "learner_fa = pyonmttok.BPELearner(tokenizer=tokenizer_persian, symbols=32000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:36.720088Z",
          "iopub.status.busy": "2021-06-30T02:50:36.719639Z",
          "iopub.status.idle": "2021-06-30T02:50:46.745242Z",
          "shell.execute_reply": "2021-06-30T02:50:46.744377Z",
          "shell.execute_reply.started": "2021-06-30T02:50:36.720048Z"
        },
        "id": "aJusyvUNZ76d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in range(len(text_fa)):\n",
        "  learner_fa.ingest(text_fa[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:46.746894Z",
          "iopub.status.busy": "2021-06-30T02:50:46.746544Z",
          "iopub.status.idle": "2021-06-30T02:50:54.171766Z",
          "shell.execute_reply": "2021-06-30T02:50:54.170879Z",
          "shell.execute_reply.started": "2021-06-30T02:50:46.746859Z"
        },
        "id": "aaj9iFwnZ76d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer_persian = learner_fa.learn(\"model_fa-32k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:50:54.192103Z",
          "iopub.status.busy": "2021-06-30T02:50:54.191512Z",
          "iopub.status.idle": "2021-06-30T02:50:54.198124Z",
          "shell.execute_reply": "2021-06-30T02:50:54.197166Z",
          "shell.execute_reply.started": "2021-06-30T02:50:54.192067Z"
        },
        "id": "b-HxWwBKZ76g",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def tokenizer_fa(text):\n",
        "  text = re.sub(\"(\\\\u200c|\\\\xad)\", \" \", text)\n",
        "  tokens, _ = tokenizer_persian.tokenize(text)\n",
        "  return tokens\n",
        "def tokenizer_eng(text):\n",
        "  tokens, _ = tokenizer_english.tokenize(text)\n",
        "  return [token.lower() for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GMIxRwfbdEc"
      },
      "source": [
        "## create Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:51:00.180446Z",
          "iopub.status.busy": "2021-06-30T02:51:00.180076Z",
          "iopub.status.idle": "2021-06-30T02:51:00.585070Z",
          "shell.execute_reply": "2021-06-30T02:51:00.584289Z",
          "shell.execute_reply.started": "2021-06-30T02:51:00.180405Z"
        },
        "id": "2p8B6EI9Z76h",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:51:00.586615Z",
          "iopub.status.busy": "2021-06-30T02:51:00.586243Z",
          "iopub.status.idle": "2021-06-30T02:51:00.591856Z",
          "shell.execute_reply": "2021-06-30T02:51:00.590738Z",
          "shell.execute_reply.started": "2021-06-30T02:51:00.586578Z"
        },
        "id": "lOH6-oA3Z76i",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "english = Field(sequential=True, use_vocab=True, tokenize=tokenizer_eng, lower=True, init_token='<SOS>', eos_token='<EOS>', batch_first=True)\n",
        "persian = Field(sequential=True, use_vocab=True, tokenize=tokenizer_fa, init_token='<SOS>', eos_token='<EOS>',  batch_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:51:00.593687Z",
          "iopub.status.busy": "2021-06-30T02:51:00.593266Z",
          "iopub.status.idle": "2021-06-30T02:51:00.604781Z",
          "shell.execute_reply": "2021-06-30T02:51:00.603948Z",
          "shell.execute_reply.started": "2021-06-30T02:51:00.593650Z"
        },
        "id": "fJ_OaUSvZ76i",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fields = {'English':('eng', english), 'Persian':('fa', persian)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:51:00.606583Z",
          "iopub.status.busy": "2021-06-30T02:51:00.606157Z",
          "iopub.status.idle": "2021-06-30T02:52:59.000743Z",
          "shell.execute_reply": "2021-06-30T02:52:58.999860Z",
          "shell.execute_reply.started": "2021-06-30T02:51:00.606487Z"
        },
        "id": "YzZaqnKtZ76j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_data, validation_data = TabularDataset.splits(\n",
        "    path='',\n",
        "    train = 'train.csv',\n",
        "    validation = 'validation.csv',\n",
        "    format='csv',\n",
        "    fields=fields\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:52:59.002449Z",
          "iopub.status.busy": "2021-06-30T02:52:59.002092Z",
          "iopub.status.idle": "2021-06-30T02:53:07.442521Z",
          "shell.execute_reply": "2021-06-30T02:53:07.441609Z",
          "shell.execute_reply.started": "2021-06-30T02:52:59.002415Z"
        },
        "id": "IMZ_stnnZ76k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "english.build_vocab(train_data,max_size= 20000, min_freq=5)\n",
        "persian.build_vocab(train_data,max_size= 20000, min_freq=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.444119Z",
          "iopub.status.busy": "2021-06-30T02:53:07.443773Z",
          "iopub.status.idle": "2021-06-30T02:53:07.448967Z",
          "shell.execute_reply": "2021-06-30T02:53:07.448033Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.444086Z"
        },
        "id": "qC2iVZfLZ76k",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_iterator, validation_iterator = BucketIterator.splits(\n",
        "    (train_data, validation_data),\n",
        "    batch_size=64,\n",
        "    sort=False,\n",
        "    shuffle=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZP7_6kVbnFn"
      },
      "source": [
        "# Create Model\n",
        "- define positional encoding, word embedding, position wise feedforward, multihead attention modules\n",
        "- create encoder layer\n",
        "- create encoder cosist of multiple encoder layers\n",
        "- create decoder cosist of multiple decoder layers usinf torch modules\n",
        "- create transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.564383Z",
          "iopub.status.busy": "2021-06-30T02:53:07.564017Z",
          "iopub.status.idle": "2021-06-30T02:53:07.611904Z",
          "shell.execute_reply": "2021-06-30T02:53:07.611065Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.564332Z"
        },
        "id": "6Np8ZhJBZ76m",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import  torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmCcxBkJbr8g"
      },
      "source": [
        "## positional encoding\n",
        "- embedding size of 256\n",
        "- dropout rate = 0.1\n",
        "- max length of positional encoding tensor = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.613740Z",
          "iopub.status.busy": "2021-06-30T02:53:07.613088Z",
          "iopub.status.idle": "2021-06-30T02:53:07.626019Z",
          "shell.execute_reply": "2021-06-30T02:53:07.625200Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.613650Z"
        },
        "id": "TqEGTBB5Z76m",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 200):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        # sum of tokekn embeding and positional encoding\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPTU_lztb3mL"
      },
      "source": [
        "## Embedding\n",
        "- map word vectors to embedding with size of 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.627572Z",
          "iopub.status.busy": "2021-06-30T02:53:07.627203Z",
          "iopub.status.idle": "2021-06-30T02:53:07.635463Z",
          "shell.execute_reply": "2021-06-30T02:53:07.634630Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.627538Z"
        },
        "id": "FEMxrhIKZ76o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        # token embedding * scale\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiYAw8oqcAKe"
      },
      "source": [
        "## position wise feed forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.638871Z",
          "iopub.status.busy": "2021-06-30T02:53:07.637848Z",
          "iopub.status.idle": "2021-06-30T02:53:07.647752Z",
          "shell.execute_reply": "2021-06-30T02:53:07.646771Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.638836Z"
        },
        "id": "a9ZDv85JZ76p",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.dropout(torch.relu(self.fc_1(x))) # (batch size, seq len, hid dim)\n",
        "        x = self.fc_2(x) # (batch size, seq len, pf dim)\n",
        "        return x # (batch size, seq len, hid dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOl19pm3cIWk"
      },
      "source": [
        "## Multihead attention\n",
        "- calculate attention by scaled dot product using key, value and Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.649343Z",
          "iopub.status.busy": "2021-06-30T02:53:07.648993Z",
          "iopub.status.idle": "2021-06-30T02:53:07.666714Z",
          "shell.execute_reply": "2021-06-30T02:53:07.665669Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.649310Z"
        },
        "id": "1zySzgmDZ76q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        #assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim # embedding size or d_model\n",
        "        self.n_heads = n_heads  # number of heads\n",
        "        self.head_dim = hid_dim // n_heads # value dim = Query dim\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim) # Query\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim) # Key\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim) # Value\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device) # scale for energy in scaled dot product\n",
        "  \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.fc_q(query) # (batch size, query len, hid dim)\n",
        "        K = self.fc_k(key) # (batch size, query len, hid dim)\n",
        "        V = self.fc_v(value) # (batch size, query len, hid dim)\n",
        "     \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # (batch size, n heads, query len, head dim)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # (batch size, n heads, key len, head dim)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3) # (batch size, n heads, value len, head dim)\n",
        "    \n",
        "        # Scaled Dot Product\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale # (batch size, n heads, query len, key len)\n",
        "        # apply mask for paddings in source sentence\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 1,  float(\"-inf\"))\n",
        "        attention = torch.softmax(energy, dim = -1) # (batch size, n heads, query len, key len)\n",
        "        x = torch.matmul(self.dropout(attention), V) # (batch size, n heads, query len, head dim)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous() # (batch size, query len, n heads, head dim)\n",
        "        x = x.view(batch_size, -1, self.hid_dim) # (batch size, query len, hid dim)\n",
        "        x = self.fc_o(x) # (batch size, query len, hid dim)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pKIoF39cNBP"
      },
      "source": [
        "## creating encoder layer\n",
        "\n",
        "- multihead attention\n",
        "- add (residual) and layer Norm\n",
        "- positionwise feed forward\n",
        "- add (residual) and layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.668605Z",
          "iopub.status.busy": "2021-06-30T02:53:07.668239Z",
          "iopub.status.idle": "2021-06-30T02:53:07.678614Z",
          "shell.execute_reply": "2021-06-30T02:53:07.677799Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.668573Z"
        },
        "id": "MEcVqOX7Z76s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim, eps = 1e-05)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim,  eps = 1e-05)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        #self attention\n",
        "        _src = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjEkRsvjcgUt"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "- token embedding + positional encoding\n",
        "- encdoer layer (multihead attention + add and norm + position wise feed forward + add and norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.680237Z",
          "iopub.status.busy": "2021-06-30T02:53:07.679894Z",
          "iopub.status.idle": "2021-06-30T02:53:07.692548Z",
          "shell.execute_reply": "2021-06-30T02:53:07.691550Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.680205Z"
        },
        "id": "PW7y0fqwZ76t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 200):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.src_tok_emb = TokenEmbedding(input_dim, hid_dim)\n",
        "        self.positional_encoding = PositionalEncoding(hid_dim, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.positional_encoding(self.src_tok_emb(src)).to(device)\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oz7ZAJlclgH"
      },
      "source": [
        "## Decoder\n",
        "- token embedding + positional encoding\n",
        "- Decoder layer of pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.694270Z",
          "iopub.status.busy": "2021-06-30T02:53:07.693850Z",
          "iopub.status.idle": "2021-06-30T02:53:07.704328Z",
          "shell.execute_reply": "2021-06-30T02:53:07.703564Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.694163Z"
        },
        "id": "8oBPl6pUZ76u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, N, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.tgt_tok_emb = TokenEmbedding(trg_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n",
        "        self.layers = nn.ModuleList([nn.TransformerDecoderLayer(embed_size, heads, batch_first=True, layer_norm_eps = 1e-05) for i in range(N)])\n",
        "    \n",
        "    def forward(self, tgt, e_outputs, tgt_key_padding_mask, tgt_mask):\n",
        "        tgt = self.positional_encoding(self.tgt_tok_emb(tgt)).to(device)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](tgt, e_outputs, tgt_mask = tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vGHkjTicpvY"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:07.706008Z",
          "iopub.status.busy": "2021-06-30T02:53:07.705672Z",
          "iopub.status.idle": "2021-06-30T02:53:07.720265Z",
          "shell.execute_reply": "2021-06-30T02:53:07.719178Z",
          "shell.execute_reply.started": "2021-06-30T02:53:07.705985Z"
        },
        "id": "aJ7SZgfSZ76w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, device):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads,pf_dim=1024, dropout=0.1, device=device)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "        self.softmax= nn.Softmax()\n",
        "        self.trg_pad_idx = 1\n",
        "        self.src_pad_idx =1\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self, src):       \n",
        "        src_mask = (src == self.src_pad_idx)\n",
        "        return src_mask\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def create_mask(self, src, tgt):\n",
        "        tgt_seq_len = tgt.shape[1]\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
        "        tgt_padding_mask = (tgt == 1)\n",
        "        return tgt_mask ,tgt_padding_mask \n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask, tgt_padding_mask = self.create_mask(src, trg)\n",
        "        e_outputs = self.encoder(src, src_mask.unsqueeze(1).unsqueeze(2))\n",
        "        d_output = self.decoder(trg, e_outputs, tgt_padding_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Mkwg5Bc6AP"
      },
      "source": [
        "# Define a transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:24.149959Z",
          "iopub.status.busy": "2021-06-30T02:53:24.149584Z",
          "iopub.status.idle": "2021-06-30T02:53:27.163575Z",
          "shell.execute_reply": "2021-06-30T02:53:27.162677Z",
          "shell.execute_reply.started": "2021-06-30T02:53:24.149923Z"
        },
        "id": "j1I_wNs3Z76z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "d_model = 256\n",
        "heads = 8\n",
        "N = 3\n",
        "src_vocab = len(english.vocab)\n",
        "trg_vocab = len(persian.vocab)\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads,device)\n",
        "# initialize with Xavier initialization to prevent gradient problems (vanishing or exploding)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:32.336303Z",
          "iopub.status.busy": "2021-06-30T02:53:32.335983Z",
          "iopub.status.idle": "2021-06-30T02:53:32.375331Z",
          "shell.execute_reply": "2021-06-30T02:53:32.374419Z",
          "shell.execute_reply.started": "2021-06-30T02:53:32.336274Z"
        },
        "id": "zuEKH9BxZ760",
        "outputId": "f082f15c-bf96-4475-816f-36f73bfefda7",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer1(\n",
              "  (encoder): Encoder(\n",
              "    (src_tok_emb): TokenEmbedding(\n",
              "      (embedding): Embedding(20004, 256)\n",
              "    )\n",
              "    (positional_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (fc_2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tgt_tok_emb): TokenEmbedding(\n",
              "      (embedding): Embedding(20004, 256)\n",
              "    )\n",
              "    (positional_encoding): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=256, out_features=20004, bias=True)\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFvWVm_gdJ8r"
      },
      "source": [
        "# Train and validate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:33.422787Z",
          "iopub.status.busy": "2021-06-30T02:53:33.422087Z",
          "iopub.status.idle": "2021-06-30T02:53:33.437518Z",
          "shell.execute_reply": "2021-06-30T02:53:33.436500Z",
          "shell.execute_reply.started": "2021-06-30T02:53:33.422734Z"
        },
        "id": "DxlzMoY6Z761",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 1) # padding index is 1 for both languages\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE,  betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:34.019674Z",
          "iopub.status.busy": "2021-06-30T02:53:34.019345Z",
          "iopub.status.idle": "2021-06-30T02:53:34.034642Z",
          "shell.execute_reply": "2021-06-30T02:53:34.031692Z",
          "shell.execute_reply.started": "2021-06-30T02:53:34.019645Z"
        },
        "id": "kV_fya5UZ762",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(epochs,max_iter):\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "    iter = 0\n",
        "    total_loss = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss=0\n",
        "        for i, batch in enumerate(train_iterator):\n",
        "            model.train()\n",
        "            iter+=1\n",
        "            src = batch.eng.to(device)\n",
        "            trg = batch.fa.to(device)\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "            output = model(src, trg[:,:-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # clip gradient to prevent vanishing\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() \n",
        "            # validate every 200 iteration\n",
        "            if iter%200 ==0 :\n",
        "              val_loss = 0\n",
        "              model.eval()\n",
        "              with torch.no_grad():\n",
        "                for j, batch in enumerate(validation_iterator):\n",
        "                  src = batch.eng.to(device)\n",
        "                  trg = batch.fa.to(device)\n",
        "                  output = model(src, trg[:,:-1])\n",
        "                  output_dim = output.shape[-1]\n",
        "                  output = output.contiguous().view(-1, output_dim)\n",
        "                  trg = trg[:,1:].contiguous().view(-1)\n",
        "                  loss = criterion(output, trg)\n",
        "                  val_loss += loss.item()\n",
        "                # print train and validation loss\n",
        "                print(f' epoch:{epoch} - iteration:{iter} - train loss:{epoch_loss/(i+1)} - val loss:{val_loss/(j+1)}')\n",
        "                torch.save(model.cpu().state_dict(), 'final_model.pth') # saving model\n",
        "                model.to(device)\n",
        "                train_loss.append(epoch_loss/(i+1))\n",
        "                valid_loss.append(val_loss/(j+1))\n",
        "            # fininsh training if reach max iteration\n",
        "            if iter==max_iter :\n",
        "              print('finish training!')\n",
        "              break\n",
        "        if iter==max_iter:\n",
        "          break\n",
        "    return train_loss, valid_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T02:53:35.844503Z",
          "iopub.status.busy": "2021-06-30T02:53:35.844173Z",
          "iopub.status.idle": "2021-06-30T08:10:13.237159Z",
          "shell.execute_reply": "2021-06-30T08:10:13.235608Z",
          "shell.execute_reply.started": "2021-06-30T02:53:35.844474Z"
        },
        "id": "3pQI587IZ763",
        "outputId": "e04be6dd-73bb-433a-9c11-98b54e632430",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " epoch:0 - iteration:200 - train loss:7.896258006095886 - val loss:6.890741557718437\n",
            " epoch:0 - iteration:400 - train loss:7.264234791994095 - val loss:6.330566448140367\n",
            " epoch:0 - iteration:600 - train loss:6.89092343489329 - val loss:5.958626418247401\n",
            " epoch:0 - iteration:800 - train loss:6.62704486489296 - val loss:5.735162189964936\n",
            " epoch:0 - iteration:1000 - train loss:6.434420331478119 - val loss:5.58832673179769\n",
            " epoch:0 - iteration:1200 - train loss:6.28602364619573 - val loss:5.470906145327559\n",
            " epoch:0 - iteration:1400 - train loss:6.163895274911608 - val loss:5.359839007119152\n",
            " epoch:0 - iteration:1600 - train loss:6.05933445096016 - val loss:5.282641337296673\n",
            " epoch:0 - iteration:1800 - train loss:5.9699502046902975 - val loss:5.197603493093331\n",
            " epoch:0 - iteration:2000 - train loss:5.889912142515183 - val loss:5.131369146454\n",
            " epoch:0 - iteration:2200 - train loss:5.818554910312999 - val loss:5.073108841994099\n",
            " epoch:0 - iteration:2400 - train loss:5.75427679002285 - val loss:5.020258332635755\n",
            " epoch:0 - iteration:2600 - train loss:5.695569794361408 - val loss:4.963562600857744\n",
            " epoch:0 - iteration:2800 - train loss:5.642422989436558 - val loss:4.919601557187945\n",
            " epoch:0 - iteration:3000 - train loss:5.593470301946004 - val loss:4.877533001320384\n",
            " epoch:0 - iteration:3200 - train loss:5.548925797045231 - val loss:4.839193395810707\n",
            " epoch:0 - iteration:3400 - train loss:5.506819866124322 - val loss:4.795368901814256\n",
            " epoch:0 - iteration:3600 - train loss:5.466699159675175 - val loss:4.765979186173912\n",
            " epoch:0 - iteration:3800 - train loss:5.428846228248195 - val loss:4.728658399849294\n",
            " epoch:0 - iteration:4000 - train loss:5.393153564453125 - val loss:4.690288637731677\n",
            " epoch:0 - iteration:4200 - train loss:5.359946758974166 - val loss:4.660337678962779\n",
            " epoch:0 - iteration:4400 - train loss:5.327864005782388 - val loss:4.6357868653591545\n",
            " epoch:0 - iteration:4600 - train loss:5.297140539003455 - val loss:4.59759872917817\n",
            " epoch:0 - iteration:4800 - train loss:5.26728651245435 - val loss:4.569086793649976\n",
            " epoch:0 - iteration:5000 - train loss:5.239714911842346 - val loss:4.544197734048433\n",
            " epoch:0 - iteration:5200 - train loss:5.212492149793185 - val loss:4.517340040206909\n",
            " epoch:0 - iteration:5400 - train loss:5.186561449103885 - val loss:4.497154005888467\n",
            " epoch:0 - iteration:5600 - train loss:5.1614462285382405 - val loss:4.467426576346995\n",
            " epoch:0 - iteration:5800 - train loss:5.136937055998835 - val loss:4.4483181599144626\n",
            " epoch:0 - iteration:6000 - train loss:5.113523362874985 - val loss:4.423132962378386\n",
            " epoch:0 - iteration:6200 - train loss:5.091300458677353 - val loss:4.3952601227804875\n",
            " epoch:0 - iteration:6400 - train loss:5.068908854871989 - val loss:4.372886786505441\n",
            " epoch:0 - iteration:6600 - train loss:5.048144337263975 - val loss:4.349694427597188\n",
            " epoch:0 - iteration:6800 - train loss:5.027756900997723 - val loss:4.328164205818533\n",
            " epoch:0 - iteration:7000 - train loss:5.0073882491588595 - val loss:4.314255310887488\n",
            " epoch:0 - iteration:7200 - train loss:4.988378848069244 - val loss:4.289533495903015\n",
            " epoch:0 - iteration:7400 - train loss:4.969327373633513 - val loss:4.268204527034937\n",
            " epoch:0 - iteration:7600 - train loss:4.9505835864418435 - val loss:4.248509776480844\n",
            " epoch:0 - iteration:7800 - train loss:4.932390415974153 - val loss:4.232862524005855\n",
            " epoch:0 - iteration:8000 - train loss:4.91452581217885 - val loss:4.212910507104107\n",
            " epoch:0 - iteration:8200 - train loss:4.897380335040209 - val loss:4.193420999072423\n",
            " epoch:0 - iteration:8400 - train loss:4.880943979734466 - val loss:4.1773141580207325\n",
            " epoch:0 - iteration:8600 - train loss:4.864532598977865 - val loss:4.1535037528688665\n",
            " epoch:0 - iteration:8800 - train loss:4.848141639097171 - val loss:4.13912036062401\n",
            " epoch:0 - iteration:9000 - train loss:4.832267441723082 - val loss:4.120094756322486\n",
            " epoch:0 - iteration:9200 - train loss:4.81670747531497 - val loss:4.102500828404293\n",
            " epoch:0 - iteration:9400 - train loss:4.801516013880994 - val loss:4.078819038489154\n",
            " epoch:0 - iteration:9600 - train loss:4.786710989624262 - val loss:4.0684409658485485\n",
            " epoch:1 - iteration:9800 - train loss:3.9991510581970213 - val loss:4.043779894793145\n",
            " epoch:1 - iteration:10000 - train loss:3.9814987907409667 - val loss:4.031346490227174\n",
            " epoch:1 - iteration:10200 - train loss:3.9783688984746517 - val loss:4.01713152600226\n",
            " epoch:1 - iteration:10400 - train loss:3.9726327068574965 - val loss:4.008882761224408\n",
            " epoch:1 - iteration:10600 - train loss:3.970996748361832 - val loss:3.9917007040754657\n",
            " epoch:1 - iteration:10800 - train loss:3.9646274757385256 - val loss:3.978721737638812\n",
            " epoch:1 - iteration:11000 - train loss:3.959389430652965 - val loss:3.969101812237891\n",
            " epoch:1 - iteration:11200 - train loss:3.952773740405128 - val loss:3.955647462996367\n",
            " epoch:1 - iteration:11400 - train loss:3.946470054169776 - val loss:3.9397760351127555\n",
            " epoch:1 - iteration:11600 - train loss:3.9409277895432484 - val loss:3.93144411751043\n",
            " epoch:1 - iteration:11800 - train loss:3.9343023137936646 - val loss:3.914106952809842\n",
            " epoch:1 - iteration:12000 - train loss:3.928732799931576 - val loss:3.911989504823061\n",
            " epoch:1 - iteration:12200 - train loss:3.9229843288717916 - val loss:3.8855792346401747\n",
            " epoch:1 - iteration:12400 - train loss:3.917225884102486 - val loss:3.882466454372228\n",
            " epoch:1 - iteration:12600 - train loss:3.9113840429322058 - val loss:3.864316181601765\n",
            " epoch:1 - iteration:12800 - train loss:3.9063054892772766 - val loss:3.8582107312211367\n",
            " epoch:1 - iteration:13000 - train loss:3.9006698255185728 - val loss:3.842003599059916\n",
            " epoch:1 - iteration:13200 - train loss:3.8951180166631314 - val loss:3.8333268375040213\n",
            " epoch:1 - iteration:13400 - train loss:3.8902125724893533 - val loss:3.82237305574328\n",
            " epoch:1 - iteration:13600 - train loss:3.88526504708536 - val loss:3.816445266420596\n",
            " epoch:1 - iteration:13800 - train loss:3.8795191883612534 - val loss:3.806321464966391\n",
            " epoch:1 - iteration:14000 - train loss:3.874399134881156 - val loss:3.798734446329491\n",
            " epoch:1 - iteration:14200 - train loss:3.8693302426051575 - val loss:3.787076429117506\n",
            " epoch:1 - iteration:14400 - train loss:3.863950193465068 - val loss:3.7742021984029037\n",
            " epoch:1 - iteration:14600 - train loss:3.8591361612770427 - val loss:3.7680337221823006\n",
            " epoch:1 - iteration:14800 - train loss:3.8543183962043357 - val loss:3.7600416107712507\n",
            " epoch:1 - iteration:15000 - train loss:3.8498072710702584 - val loss:3.7499365944728673\n",
            " epoch:1 - iteration:15200 - train loss:3.8453471608439904 - val loss:3.7465098909128494\n",
            " epoch:1 - iteration:15400 - train loss:3.840635739404918 - val loss:3.7421852419309527\n",
            " epoch:1 - iteration:15600 - train loss:3.8361475450124702 - val loss:3.7260235993661612\n",
            " epoch:1 - iteration:15800 - train loss:3.8312675422405906 - val loss:3.7187006700818785\n",
            " epoch:1 - iteration:16000 - train loss:3.8273768105974386 - val loss:3.70761071298724\n",
            " epoch:1 - iteration:16200 - train loss:3.8228836048329287 - val loss:3.7033922115218973\n",
            " epoch:1 - iteration:16400 - train loss:3.8182686352465867 - val loss:3.693203366582639\n",
            " epoch:1 - iteration:16600 - train loss:3.814488678901426 - val loss:3.683825085764733\n",
            " epoch:1 - iteration:16800 - train loss:3.8102164841362822 - val loss:3.674489843956778\n",
            " epoch:1 - iteration:17000 - train loss:3.80602575318288 - val loss:3.672660297322496\n",
            " epoch:1 - iteration:17200 - train loss:3.8018503613676566 - val loss:3.6654297737317663\n",
            " epoch:1 - iteration:17400 - train loss:3.798008631188003 - val loss:3.659969331616553\n",
            " epoch:1 - iteration:17600 - train loss:3.7940189627345453 - val loss:3.6498715079833413\n",
            " epoch:1 - iteration:17800 - train loss:3.789928330033562 - val loss:3.649773350608683\n",
            " epoch:1 - iteration:18000 - train loss:3.785842061711781 - val loss:3.634197715732539\n",
            " epoch:1 - iteration:18200 - train loss:3.7817490815977313 - val loss:3.6311107094042767\n",
            " epoch:1 - iteration:18400 - train loss:3.7781303904606744 - val loss:3.6249018822874977\n",
            " epoch:1 - iteration:18600 - train loss:3.774461256946362 - val loss:3.625191866571658\n",
            " epoch:1 - iteration:18800 - train loss:3.7706688985902543 - val loss:3.616335883541642\n",
            " epoch:1 - iteration:19000 - train loss:3.766983882776896 - val loss:3.6123690850266787\n",
            " epoch:1 - iteration:19200 - train loss:3.763328811954269 - val loss:3.6044764788351324\n",
            " epoch:2 - iteration:19400 - train loss:3.49140439192454 - val loss:3.6024891298507975\n",
            " epoch:2 - iteration:19600 - train loss:3.4775355652400424 - val loss:3.5929191952553863\n",
            " epoch:2 - iteration:19800 - train loss:3.4754993104934693 - val loss:3.587261694614018\n",
            " epoch:2 - iteration:20000 - train loss:3.4727262659072875 - val loss:3.5842119535553123\n",
            " epoch:2 - iteration:20200 - train loss:3.4748136856681424 - val loss:3.580032489232928\n",
            " epoch:2 - iteration:20400 - train loss:3.475059123661207 - val loss:3.5820427132544115\n",
            " epoch:2 - iteration:20600 - train loss:3.4758729078151562 - val loss:3.5726554235565327\n",
            " epoch:2 - iteration:20800 - train loss:3.473731766362344 - val loss:3.5783245686058684\n",
            " epoch:2 - iteration:21000 - train loss:3.4741567262922013 - val loss:3.5644416439199\n",
            " epoch:2 - iteration:21200 - train loss:3.4712612926042996 - val loss:3.5607744170126514\n",
            " epoch:2 - iteration:21400 - train loss:3.4693739715842313 - val loss:3.560155007995178\n",
            " epoch:2 - iteration:21600 - train loss:3.468446637315953 - val loss:3.5545465346808744\n",
            " epoch:2 - iteration:21800 - train loss:3.467130458308201 - val loss:3.545930574764715\n",
            " epoch:2 - iteration:22000 - train loss:3.46532385522669 - val loss:3.5427294920538075\n",
            " epoch:2 - iteration:22200 - train loss:3.463581704931744 - val loss:3.536893125560796\n",
            " epoch:2 - iteration:22400 - train loss:3.4619406241462345 - val loss:3.535760819577725\n",
            " epoch:2 - iteration:22600 - train loss:3.4608310456774127 - val loss:3.5274410492905948\n",
            " epoch:2 - iteration:22800 - train loss:3.459008649906642 - val loss:3.522159065487229\n",
            " epoch:2 - iteration:23000 - train loss:3.459586800320943 - val loss:3.520719802936661\n",
            " epoch:2 - iteration:23200 - train loss:3.4582523554186277 - val loss:3.515853704024698\n",
            " epoch:2 - iteration:23400 - train loss:3.4568259767164666 - val loss:3.5121261772708356\n",
            " epoch:2 - iteration:23600 - train loss:3.4553727434969495 - val loss:3.509036660640039\n",
            " epoch:2 - iteration:23800 - train loss:3.4544779096330913 - val loss:3.5026705991441958\n",
            " epoch:2 - iteration:24000 - train loss:3.4534680175279315 - val loss:3.5030367893593333\n",
            " epoch:2 - iteration:24200 - train loss:3.452548463272326 - val loss:3.5016187398233147\n",
            " epoch:2 - iteration:24400 - train loss:3.450996857846825 - val loss:3.490730123430769\n",
            " epoch:2 - iteration:24600 - train loss:3.4496467598799234 - val loss:3.4962819438114345\n",
            " epoch:2 - iteration:24800 - train loss:3.4479840645918975 - val loss:3.4864064078464687\n",
            " epoch:2 - iteration:25000 - train loss:3.446802510800569 - val loss:3.488029968627145\n",
            " epoch:2 - iteration:25200 - train loss:3.4456078799432066 - val loss:3.484858034927154\n",
            " epoch:2 - iteration:25400 - train loss:3.4442361170295777 - val loss:3.4751216335831403\n",
            " epoch:2 - iteration:25600 - train loss:3.442719155033742 - val loss:3.471563480056335\n",
            " epoch:2 - iteration:25800 - train loss:3.4410831158397763 - val loss:3.47156194036252\n",
            " epoch:2 - iteration:26000 - train loss:3.4399050801241837 - val loss:3.466189663878111\n",
            " epoch:2 - iteration:26200 - train loss:3.438540121394096 - val loss:3.4613298549830356\n",
            " epoch:2 - iteration:26400 - train loss:3.4372574439415566 - val loss:3.457099199295044\n",
            " epoch:2 - iteration:26600 - train loss:3.4362168866436495 - val loss:3.4547103387173093\n",
            " epoch:2 - iteration:26800 - train loss:3.435023529671675 - val loss:3.4548868362034595\n",
            " epoch:2 - iteration:27000 - train loss:3.434114480510835 - val loss:3.4520322603600047\n",
            " epoch:2 - iteration:27200 - train loss:3.432520811662734 - val loss:3.4486964098761015\n",
            " epoch:2 - iteration:27400 - train loss:3.4315427440514594 - val loss:3.4415039866884176\n",
            " epoch:2 - iteration:27600 - train loss:3.430031440814812 - val loss:3.4423579474475896\n",
            " epoch:2 - iteration:27800 - train loss:3.42836076842414 - val loss:3.4356228188933615\n",
            " epoch:2 - iteration:28000 - train loss:3.426979182815552 - val loss:3.4316227935184944\n",
            " epoch:2 - iteration:28200 - train loss:3.425843273860782 - val loss:3.435352107074773\n",
            " epoch:2 - iteration:28400 - train loss:3.4248993908772705 - val loss:3.433982559008019\n",
            " epoch:2 - iteration:28600 - train loss:3.4235742820392954 - val loss:3.4226627216160854\n",
            " epoch:2 - iteration:28800 - train loss:3.4222697806233513 - val loss:3.4155080525674553\n",
            " epoch:3 - iteration:29000 - train loss:3.240861770629883 - val loss:3.4148725863929106\n",
            " epoch:3 - iteration:29200 - train loss:3.239439588693472 - val loss:3.4183847041887656\n",
            " epoch:3 - iteration:29400 - train loss:3.2330892217726936 - val loss:3.4163201706431736\n",
            " epoch:3 - iteration:29600 - train loss:3.2392822370858028 - val loss:3.4129912929000144\n",
            " epoch:3 - iteration:29800 - train loss:3.2415029654631744 - val loss:3.4099923512645973\n",
            " epoch:3 - iteration:30000 - train loss:3.2392149397532144 - val loss:3.4107835259392996\n",
            " epoch:3 - iteration:30200 - train loss:3.237756554405644 - val loss:3.4140673256366054\n",
            " epoch:3 - iteration:30400 - train loss:3.240188774046351 - val loss:3.404561544133124\n",
            " epoch:3 - iteration:30600 - train loss:3.239924360634624 - val loss:3.4055736604137956\n",
            " epoch:3 - iteration:30800 - train loss:3.240162312520015 - val loss:3.3985603426104394\n",
            " epoch:3 - iteration:31000 - train loss:3.2408848505581127 - val loss:3.3991284435040483\n",
            " epoch:3 - iteration:31200 - train loss:3.2413482476306217 - val loss:3.4003894761343982\n",
            " epoch:3 - iteration:31400 - train loss:3.2438468135229432 - val loss:3.397817994055347\n",
            " epoch:3 - iteration:31600 - train loss:3.24531891280358 - val loss:3.390373235773817\n",
            " epoch:3 - iteration:31800 - train loss:3.2449630189553287 - val loss:3.3939560108095685\n",
            " epoch:3 - iteration:32000 - train loss:3.2435521625518797 - val loss:3.386919678037412\n",
            " epoch:3 - iteration:32200 - train loss:3.243748588418602 - val loss:3.3910691466286917\n",
            " epoch:3 - iteration:32400 - train loss:3.243760487982567 - val loss:3.383909473909396\n",
            " epoch:3 - iteration:32600 - train loss:3.2443135091922426 - val loss:3.38106391407619\n",
            " epoch:3 - iteration:32800 - train loss:3.245580524395985 - val loss:3.379930151957218\n",
            " epoch:3 - iteration:33000 - train loss:3.246339674573956 - val loss:3.3786533803583305\n",
            " epoch:3 - iteration:33200 - train loss:3.245960613372009 - val loss:3.3802080854077206\n",
            " epoch:3 - iteration:33400 - train loss:3.2464543944300868 - val loss:3.382459332341346\n",
            " epoch:3 - iteration:33600 - train loss:3.2465554202044453 - val loss:3.3711650638936836\n",
            " epoch:3 - iteration:33800 - train loss:3.246213466334464 - val loss:3.3659417956788964\n",
            " epoch:3 - iteration:34000 - train loss:3.2457609061729618 - val loss:3.368449376676684\n",
            " epoch:3 - iteration:34200 - train loss:3.2462463047135044 - val loss:3.3697386902069377\n",
            " epoch:3 - iteration:34400 - train loss:3.2459238822643575 - val loss:3.3767004255936524\n",
            " epoch:3 - iteration:34600 - train loss:3.246046750597558 - val loss:3.363005358927718\n",
            " epoch:3 - iteration:34800 - train loss:3.245283742695418 - val loss:3.362935773234501\n",
            " epoch:3 - iteration:35000 - train loss:3.244452730840566 - val loss:3.357062709665744\n",
            " epoch:3 - iteration:35200 - train loss:3.244110739108602 - val loss:3.3541010658317636\n",
            " epoch:3 - iteration:35400 - train loss:3.243331389810847 - val loss:3.3586716110461228\n",
            " epoch:3 - iteration:35600 - train loss:3.2429429971418418 - val loss:3.3460706470168637\n",
            " epoch:3 - iteration:35800 - train loss:3.2423537115241645 - val loss:3.34363369652044\n",
            " epoch:3 - iteration:36000 - train loss:3.2421112635428444 - val loss:3.3415230940435534\n",
            " epoch:3 - iteration:36200 - train loss:3.2419309892752066 - val loss:3.343845661778316\n",
            " epoch:3 - iteration:36400 - train loss:3.2412011570154236 - val loss:3.3466572351544817\n",
            " epoch:3 - iteration:36600 - train loss:3.2411526340496963 - val loss:3.342102295884462\n",
            " epoch:3 - iteration:36800 - train loss:3.240977698515642 - val loss:3.3363625840605975\n",
            " epoch:3 - iteration:37000 - train loss:3.240659659576416 - val loss:3.3320084582979432\n",
            " epoch:3 - iteration:37200 - train loss:3.2404527691677885 - val loss:3.33690723815811\n",
            " epoch:3 - iteration:37400 - train loss:3.240264373370979 - val loss:3.341729644748652\n",
            " epoch:3 - iteration:37600 - train loss:3.2396162103308646 - val loss:3.3352268341545748\n",
            " epoch:3 - iteration:37800 - train loss:3.2389083316012255 - val loss:3.3272182538130575\n",
            " epoch:3 - iteration:38000 - train loss:3.23849315421222 - val loss:3.3277602340573464\n",
            " epoch:3 - iteration:38200 - train loss:3.2385806675059547 - val loss:3.3237894601911027\n",
            " epoch:3 - iteration:38400 - train loss:3.2379786436576543 - val loss:3.324481717225547\n",
            " epoch:4 - iteration:38600 - train loss:3.0908305144309995 - val loss:3.3248915912949037\n",
            " epoch:4 - iteration:38800 - train loss:3.0900504851341246 - val loss:3.3344702992483835\n",
            " epoch:4 - iteration:39000 - train loss:3.085150921344757 - val loss:3.3289586414800625\n",
            " epoch:4 - iteration:39200 - train loss:3.0949688805852618 - val loss:3.3296333143644246\n",
            " epoch:4 - iteration:39400 - train loss:3.096876912381914 - val loss:3.325858864605984\n",
            " epoch:4 - iteration:39600 - train loss:3.0954001073403794 - val loss:3.3242956493502467\n",
            " epoch:4 - iteration:39800 - train loss:3.0970921536592337 - val loss:3.3285319539988154\n",
            " epoch:4 - iteration:40000 - train loss:3.098774351119995 - val loss:3.31389609653259\n",
            " epoch:4 - iteration:40200 - train loss:3.09954464449602 - val loss:3.316364179147738\n",
            " epoch:4 - iteration:40400 - train loss:3.1003668609418367 - val loss:3.319215764063541\n",
            " epoch:4 - iteration:40600 - train loss:3.1018323601995195 - val loss:3.314041235513776\n",
            " epoch:4 - iteration:40800 - train loss:3.1018836373868197 - val loss:3.311594878847354\n",
            " epoch:4 - iteration:41000 - train loss:3.1020144974708557 - val loss:3.3092000364143157\n",
            " epoch:4 - iteration:41200 - train loss:3.102229733378799 - val loss:3.311485555684455\n",
            " epoch:4 - iteration:41400 - train loss:3.1030520915162976 - val loss:3.309931581711101\n",
            " epoch:4 - iteration:41600 - train loss:3.1050945939556245 - val loss:3.3041357136218346\n",
            " epoch:4 - iteration:41800 - train loss:3.1068306790698657 - val loss:3.311548272694383\n",
            " epoch:4 - iteration:42000 - train loss:3.107273344857352 - val loss:3.305881089807671\n",
            " epoch:4 - iteration:42200 - train loss:3.107633967399597 - val loss:3.3019392104906458\n",
            " epoch:4 - iteration:42400 - train loss:3.1090190622745415 - val loss:3.302262413390329\n",
            " epoch:4 - iteration:42600 - train loss:3.1101861369900585 - val loss:3.2989624186096904\n",
            " epoch:4 - iteration:42800 - train loss:3.110286281607872 - val loss:3.302331190465767\n",
            " epoch:4 - iteration:43000 - train loss:3.1106705527835423 - val loss:3.293801010880515\n",
            " epoch:4 - iteration:43200 - train loss:3.1111435554889924 - val loss:3.2951477358274373\n",
            " epoch:4 - iteration:43400 - train loss:3.1110747702754273 - val loss:3.293254737096412\n",
            " epoch:4 - iteration:43600 - train loss:3.110893323140986 - val loss:3.296552199738048\n",
            " epoch:4 - iteration:43800 - train loss:3.11105763777247 - val loss:3.2883535229157066\n",
            " epoch:4 - iteration:44000 - train loss:3.11148569466851 - val loss:3.286421851799867\n",
            " epoch:4 - iteration:44200 - train loss:3.1122341927310875 - val loss:3.2905525125075723\n",
            " epoch:4 - iteration:44400 - train loss:3.112656112121323 - val loss:3.2866699205380736\n",
            " epoch:4 - iteration:44600 - train loss:3.1132653430250823 - val loss:3.283613524704336\n",
            " epoch:4 - iteration:44800 - train loss:3.113397594860622 - val loss:3.283852018374149\n",
            " epoch:4 - iteration:45000 - train loss:3.113651661799504 - val loss:3.2779448736493833\n",
            " epoch:4 - iteration:45200 - train loss:3.1142392053888805 - val loss:3.291556213726507\n",
            " epoch:4 - iteration:45400 - train loss:3.1146356089218803 - val loss:3.279552687439963\n",
            " epoch:4 - iteration:45600 - train loss:3.1150264179538674 - val loss:3.278333819469559\n",
            " epoch:4 - iteration:45800 - train loss:3.1149806373086695 - val loss:3.2771346845359446\n",
            " epoch:4 - iteration:46000 - train loss:3.1151644112904866 - val loss:3.2802702364520493\n",
            " epoch:4 - iteration:46200 - train loss:3.1153902258501422 - val loss:3.2754839830309432\n",
            " epoch:4 - iteration:46400 - train loss:3.1157514383219467 - val loss:3.2770437853358616\n",
            " epoch:4 - iteration:46600 - train loss:3.116153139891448 - val loss:3.2752698492781023\n",
            " epoch:4 - iteration:46800 - train loss:3.1168493940456803 - val loss:3.268437218666077\n",
            " epoch:4 - iteration:47000 - train loss:3.116734192427467 - val loss:3.2807968957401883\n",
            " epoch:4 - iteration:47200 - train loss:3.116609623267733 - val loss:3.271954249444409\n",
            " epoch:4 - iteration:47400 - train loss:3.1169224095076657 - val loss:3.2645551897655025\n",
            " epoch:4 - iteration:47600 - train loss:3.116623540055621 - val loss:3.2663412062921258\n",
            " epoch:4 - iteration:47800 - train loss:3.116628306988747 - val loss:3.2622902279702304\n",
            " epoch:4 - iteration:48000 - train loss:3.116541070461273 - val loss:3.2675622583549715\n",
            " epoch:5 - iteration:48200 - train loss:2.9776805718739827 - val loss:3.261116682926071\n",
            " epoch:5 - iteration:48400 - train loss:2.985248146057129 - val loss:3.2663752872253133\n",
            " epoch:5 - iteration:48600 - train loss:2.990400554757369 - val loss:3.26834310972802\n",
            " epoch:5 - iteration:48800 - train loss:2.9913781307361744 - val loss:3.268456018750913\n",
            " epoch:5 - iteration:49000 - train loss:2.990133385794503 - val loss:3.2672394035018493\n",
            " epoch:5 - iteration:49200 - train loss:2.989510039617849 - val loss:3.2682084928049107\n",
            " epoch:5 - iteration:49400 - train loss:2.991100315767176 - val loss:3.269276351126555\n",
            " epoch:5 - iteration:49600 - train loss:2.9941726682953917 - val loss:3.262862152919591\n",
            " epoch:5 - iteration:49800 - train loss:2.9952710124627866 - val loss:3.2661885136755826\n",
            " epoch:5 - iteration:50000 - train loss:2.996503227742513 - val loss:3.275494528262415\n",
            " epoch:5 - iteration:50200 - train loss:2.99824912726161 - val loss:3.2680121341598367\n",
            " epoch:5 - iteration:50400 - train loss:3.000927414422507 - val loss:3.2647896826824296\n",
            " epoch:5 - iteration:50600 - train loss:3.0031881223543726 - val loss:3.2597402853386424\n",
            " epoch:5 - iteration:50800 - train loss:3.0039164025315617 - val loss:3.2614748700756895\n",
            " epoch:5 - iteration:51000 - train loss:3.0060083660457444 - val loss:3.271506341818337\n",
            " epoch:5 - iteration:51200 - train loss:3.0075669000206924 - val loss:3.2557054878395295\n",
            " epoch:5 - iteration:51400 - train loss:3.008654606142117 - val loss:3.257812532977523\n",
            " epoch:5 - iteration:51600 - train loss:3.008704135194957 - val loss:3.2508202637467427\n",
            " epoch:5 - iteration:51800 - train loss:3.0089258217325017 - val loss:3.2562978806896745\n",
            " epoch:5 - iteration:52000 - train loss:3.009486070386825 - val loss:3.2586988079213652\n",
            " epoch:5 - iteration:52200 - train loss:3.0105528918073223 - val loss:3.254196729392649\n",
            " epoch:5 - iteration:52400 - train loss:3.0121716837297408 - val loss:3.2531925655971063\n",
            " epoch:5 - iteration:52600 - train loss:3.012999277541091 - val loss:3.2561406790653122\n",
            " epoch:5 - iteration:52800 - train loss:3.01404705470896 - val loss:3.2494920813034627\n",
            " epoch:5 - iteration:53000 - train loss:3.0148699741363525 - val loss:3.249173254833043\n",
            " epoch:5 - iteration:53200 - train loss:3.0164847544026494 - val loss:3.2446033876632976\n",
            " epoch:5 - iteration:53400 - train loss:3.0172873632150803 - val loss:3.2470695731795836\n",
            " epoch:5 - iteration:53600 - train loss:3.0182216160917936 - val loss:3.246942597906166\n",
            " epoch:5 - iteration:53800 - train loss:3.018925149724347 - val loss:3.2478325324637867\n",
            " epoch:5 - iteration:54000 - train loss:3.0197669661501623 - val loss:3.24895587925599\n",
            " epoch:5 - iteration:54200 - train loss:3.020437796635883 - val loss:3.241182148345163\n",
            " epoch:5 - iteration:54400 - train loss:3.020345834640868 - val loss:3.2434552776479277\n",
            " epoch:5 - iteration:54600 - train loss:3.020425588136474 - val loss:3.2363984812085875\n",
            " epoch:5 - iteration:54800 - train loss:3.0209565370538263 - val loss:3.238558156690865\n",
            " epoch:5 - iteration:55000 - train loss:3.0214622472242874 - val loss:3.2421910009651542\n",
            " epoch:5 - iteration:55200 - train loss:3.0220604840605505 - val loss:3.234614879394246\n",
            " epoch:5 - iteration:55400 - train loss:3.0224421037759157 - val loss:3.2346267443951047\n",
            " epoch:5 - iteration:55600 - train loss:3.0229444170157644 - val loss:3.2334807431586436\n",
            " epoch:5 - iteration:55800 - train loss:3.0234888641531383 - val loss:3.232163958460371\n",
            " epoch:5 - iteration:56000 - train loss:3.0238121742369635 - val loss:3.237899826843048\n",
            " epoch:5 - iteration:56200 - train loss:3.024597487582511 - val loss:3.2362242219604065\n",
            " epoch:5 - iteration:56400 - train loss:3.025106866871122 - val loss:3.2272945421878423\n",
            " epoch:5 - iteration:56600 - train loss:3.025340417957587 - val loss:3.2331554459634226\n",
            " epoch:5 - iteration:56800 - train loss:3.0257983248927753 - val loss:3.225445659138332\n",
            " epoch:5 - iteration:57000 - train loss:3.0261841327774692 - val loss:3.2279696250630314\n",
            " epoch:5 - iteration:57200 - train loss:3.0267997749580826 - val loss:3.227374576185351\n",
            " epoch:5 - iteration:57400 - train loss:3.0266132683509763 - val loss:3.2226434237489077\n",
            " epoch:5 - iteration:57600 - train loss:3.0269510067610126 - val loss:3.2247503949102954\n",
            " epoch:6 - iteration:57800 - train loss:2.9015836620330813 - val loss:3.2244762899719666\n",
            " epoch:6 - iteration:58000 - train loss:2.902743992805481 - val loss:3.231069346231835\n",
            " epoch:6 - iteration:58200 - train loss:2.8976507753796046 - val loss:3.2276591893668485\n",
            " epoch:6 - iteration:58400 - train loss:2.903431364939763 - val loss:3.2360034786652183\n",
            " epoch:6 - iteration:58600 - train loss:2.905559756615583 - val loss:3.2340298514499843\n",
            " epoch:6 - iteration:58800 - train loss:2.9082893809818087 - val loss:3.249301602684449\n",
            " epoch:6 - iteration:59000 - train loss:2.909717603492737 - val loss:3.2298239857236917\n",
            " epoch:6 - iteration:59200 - train loss:2.913187961742796 - val loss:3.231938319117109\n",
            " epoch:6 - iteration:59400 - train loss:2.9173998883276275 - val loss:3.229688898425236\n",
            " epoch:6 - iteration:59600 - train loss:2.9194634312552377 - val loss:3.236943205494747\n",
            " epoch:6 - iteration:59800 - train loss:2.9205062683617196 - val loss:3.2371688031704626\n",
            " epoch:6 - iteration:60000 - train loss:2.9201785587734648 - val loss:3.2261228438849763\n",
            " epoch:6 - iteration:60200 - train loss:2.9214345426948705 - val loss:3.230591675277068\n",
            " epoch:6 - iteration:60400 - train loss:2.923069731874286 - val loss:3.2268827683457704\n",
            " epoch:6 - iteration:60600 - train loss:2.9245491811685396 - val loss:3.2246919081589884\n",
            " epoch:6 - iteration:60800 - train loss:2.925707354311083 - val loss:3.2317349362596173\n",
            " epoch:6 - iteration:61000 - train loss:2.927913231556232 - val loss:3.230863216435798\n",
            " epoch:6 - iteration:61200 - train loss:2.929978178825931 - val loss:3.2317337470633962\n",
            " epoch:6 - iteration:61400 - train loss:2.930893310520747 - val loss:3.2274865945922993\n",
            " epoch:6 - iteration:61600 - train loss:2.9323302896920738 - val loss:3.2292258915500107\n",
            " epoch:6 - iteration:61800 - train loss:2.9343332041045764 - val loss:3.2261420468303643\n",
            " epoch:6 - iteration:62000 - train loss:2.9348911548502303 - val loss:3.225424843190986\n",
            " epoch:6 - iteration:62200 - train loss:2.9356343354535905 - val loss:3.2260281409058615\n",
            " epoch:6 - iteration:62400 - train loss:2.9367745626613657 - val loss:3.2234293806218655\n",
            " epoch:6 - iteration:62600 - train loss:2.9374660857682375 - val loss:3.2180521289878916\n",
            " epoch:6 - iteration:62800 - train loss:2.9382417189720833 - val loss:3.228108560259097\n",
            " epoch:6 - iteration:63000 - train loss:2.9391972769328527 - val loss:3.218507019604478\n",
            " epoch:6 - iteration:63200 - train loss:2.940269867477067 - val loss:3.220155015392838\n",
            " epoch:6 - iteration:63400 - train loss:2.9416440632490986 - val loss:3.2149999879230964\n",
            " epoch:6 - iteration:63600 - train loss:2.9427259630627103 - val loss:3.222912525239392\n",
            " epoch:6 - iteration:63800 - train loss:2.943850552149055 - val loss:3.2179323726725357\n",
            " epoch:6 - iteration:64000 - train loss:2.9448583647155764 - val loss:3.2149515800386945\n",
            " epoch:6 - iteration:64200 - train loss:2.946006175492161 - val loss:3.2129497646171354\n",
            " epoch:6 - iteration:64400 - train loss:2.9465552196646096 - val loss:3.2172648793069003\n",
            " epoch:6 - iteration:64600 - train loss:2.947794861723907 - val loss:3.2064846996949097\n",
            " epoch:6 - iteration:64800 - train loss:2.948360899965814 - val loss:3.2108839935231432\n",
            " epoch:6 - iteration:65000 - train loss:2.949142103984438 - val loss:3.2112901170677115\n",
            " epoch:6 - iteration:65200 - train loss:2.949972677838882 - val loss:3.2092449188232424\n",
            " epoch:6 - iteration:65400 - train loss:2.9508035934672634 - val loss:3.208467827110647\n",
            " epoch:6 - iteration:65600 - train loss:2.9518334892600966 - val loss:3.206807434670279\n",
            " epoch:6 - iteration:65800 - train loss:2.952425088290102 - val loss:3.1999954335043364\n",
            " epoch:6 - iteration:66000 - train loss:2.9532064927997013 - val loss:3.2058448087389224\n",
            " epoch:6 - iteration:66200 - train loss:2.9540952519411166 - val loss:3.2029464059901014\n",
            " epoch:6 - iteration:66400 - train loss:2.9545125252111797 - val loss:3.2031246096174293\n",
            " epoch:6 - iteration:66600 - train loss:2.9552854839421934 - val loss:3.2028408854921286\n",
            " epoch:6 - iteration:66800 - train loss:2.956051475883189 - val loss:3.198171712082123\n",
            " epoch:6 - iteration:67000 - train loss:2.9564500241923977 - val loss:3.2064357238395194\n",
            " epoch:6 - iteration:67200 - train loss:2.9571175540187373 - val loss:3.2046469786456813\n",
            " epoch:7 - iteration:67400 - train loss:2.8567341804504394 - val loss:3.2017966564570632\n",
            " epoch:7 - iteration:67600 - train loss:2.85006147702535 - val loss:3.206848095064965\n",
            " epoch:7 - iteration:67800 - train loss:2.841382517534144 - val loss:3.2047452322790555\n",
            " epoch:7 - iteration:68000 - train loss:2.8480541996002198 - val loss:3.214885986631162\n",
            "finish training!\n"
          ]
        }
      ],
      "source": [
        "train_loss, valid_loss = train_model(12, 68000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:28:27.772088Z",
          "iopub.status.busy": "2021-06-30T08:28:27.771771Z",
          "iopub.status.idle": "2021-06-30T08:28:27.928020Z",
          "shell.execute_reply": "2021-06-30T08:28:27.927042Z",
          "shell.execute_reply.started": "2021-06-30T08:28:27.772059Z"
        },
        "id": "fERO_y9cZ764",
        "outputId": "9e4b4cea-bf9c-4385-cd48-359005beba64",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "plt.title('Loss per iteration')\n",
        "plt.xlabel('iteration')\n",
        "#plt.xticks([i for i in range(200, (len(train_loss)+1)*200,200)])\n",
        "plt.ylabel('loss')\n",
        "plt.plot(train_loss, label='train')\n",
        "plt.plot(valid_loss, label='validation')\n",
        "plt.xticks(rotation=45)\n",
        "locs, labels = plt.xticks()\n",
        "labels = [(int(item))*200 for item in locs]\n",
        "plt.xticks(locs, labels)\n",
        "plt.xlim(xmin=-0.9, xmax = (len(train_loss)+1))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxGRzHFvdYzM"
      },
      "source": [
        "# Test and evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.240408Z",
          "iopub.status.busy": "2021-06-30T08:10:13.240119Z",
          "iopub.status.idle": "2021-06-30T08:10:13.270286Z",
          "shell.execute_reply": "2021-06-30T08:10:13.269548Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.240381Z"
        },
        "id": "h8vrG-nLZ764",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_text_fa0 = open('./Test/test.fa0', encoding='utf8').read().splitlines()\n",
        "test_text_fa1 = open('./Test/test.fa1', encoding='utf8').read().splitlines()\n",
        "test_text_fa2 = open('./Test/test.fa2', encoding='utf8').read().splitlines()\n",
        "test_text_fa3 = open('./Test/test.fa3', encoding='utf8').read().splitlines()\n",
        "test_text_eng =  open('./Test/test.en', encoding='utf8').read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V63ynFCXdaiK"
      },
      "source": [
        "## Translation Function\n",
        "- based on greedy search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.272376Z",
          "iopub.status.busy": "2021-06-30T08:10:13.272015Z",
          "iopub.status.idle": "2021-06-30T08:10:13.313624Z",
          "shell.execute_reply": "2021-06-30T08:10:13.312755Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.272327Z"
        },
        "id": "-Sx8OdrLZ765",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):  \n",
        "    model.eval()\n",
        "    tokens = nltk.word_tokenize(sentence.lower()) # tokenize source sentence\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token] # add <SOS> and <EOS>\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens] # map tokens to index in source vocab\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor) # make mask for if there is padding\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask) # output of encoder\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]] # strat translation with <SOS> token\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask, tgt_padding_mask = model.create_mask(src_tensor, trg_tensor) \n",
        "        with torch.no_grad():\n",
        "          output = model.decoder(trg_tensor, enc_src, tgt_padding_mask, trg_mask)\n",
        "          output = model.out(output)   \n",
        "        pred_token = output.argmax(2)[:,-1].item() # select maximum probable token due to gready search\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    return trg_tokens[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOdgIysqddpO"
      },
      "source": [
        "## test translate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.315659Z",
          "iopub.status.busy": "2021-06-30T08:10:13.315238Z",
          "iopub.status.idle": "2021-06-30T08:10:13.329398Z",
          "shell.execute_reply": "2021-06-30T08:10:13.328220Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.315624Z"
        },
        "id": "gyI43MVEZ766",
        "outputId": "9ef54174-7288-4efe-f5f0-bdb857781a07",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I did not understand , what you said about the hotel .\n"
          ]
        }
      ],
      "source": [
        "src = test_text_eng[43] # 43, 88\n",
        "print(src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.331169Z",
          "iopub.status.busy": "2021-06-30T08:10:13.330827Z",
          "iopub.status.idle": "2021-06-30T08:10:13.461935Z",
          "shell.execute_reply": "2021-06-30T08:10:13.461063Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.331136Z"
        },
        "id": "VVTcUUEaZ766",
        "outputId": "4f46695b-bbe2-44cd-c6e6-bf405017012d",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "من گفتم که هتل را متوجه شدم که شما چیست .\n"
          ]
        }
      ],
      "source": [
        "trg  = translate_sentence(src, english, persian, model, device, max_len = 50)\n",
        "print(\" \".join(trg[:-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVKIazpNdjJI"
      },
      "source": [
        "## calculate BLUE and NIST score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.463484Z",
          "iopub.status.busy": "2021-06-30T08:10:13.463136Z",
          "iopub.status.idle": "2021-06-30T08:10:13.470064Z",
          "shell.execute_reply": "2021-06-30T08:10:13.469202Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.463449Z"
        },
        "id": "gpRhkOlbZ767",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from nltk.translate import nist_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.471847Z",
          "iopub.status.busy": "2021-06-30T08:10:13.471327Z",
          "iopub.status.idle": "2021-06-30T08:10:13.550392Z",
          "shell.execute_reply": "2021-06-30T08:10:13.549586Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.471811Z"
        },
        "id": "XPntMeM8Z768",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_sentences =  [sentence for sentence in test_text_eng]\n",
        "refrences = [[tokenizer_fa(test_text_fa0[i]), tokenizer_fa(test_text_fa1[i]), tokenizer_fa(test_text_fa2[i]), tokenizer_fa(test_text_fa3[i])] for i in range(len(test_text_fa3))]\n",
        "test_results=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:10:13.552924Z",
          "iopub.status.busy": "2021-06-30T08:10:13.552571Z",
          "iopub.status.idle": "2021-06-30T08:10:24.809785Z",
          "shell.execute_reply": "2021-06-30T08:10:24.808926Z",
          "shell.execute_reply.started": "2021-06-30T08:10:13.552888Z"
        },
        "id": "1d80T44DZ768",
        "outputId": "3f3b48ed-721b-4c14-894c-6db0a1a1954b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "blue_scores = []\n",
        "Nist_score = []\n",
        "for i in range(len(test_sentences)):\n",
        "  src = test_sentences[i]\n",
        "  trg  = translate_sentence(src, english, persian, model, device, max_len = 50)\n",
        "  test_results.append(trg[:-1])\n",
        "  blue_scores.append(nltk.translate.bleu_score.corpus_bleu([refrences[i]], [trg[:-1]]))\n",
        "  try:\n",
        "    Nist_score.append(nist_score.corpus_nist([refrences[i]], [trg[:-1]]))\n",
        "  except ZeroDivisionError:\n",
        "    Nist_score.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:27:41.853138Z",
          "iopub.status.busy": "2021-06-30T08:27:41.852813Z",
          "iopub.status.idle": "2021-06-30T08:27:41.858986Z",
          "shell.execute_reply": "2021-06-30T08:27:41.857839Z",
          "shell.execute_reply.started": "2021-06-30T08:27:41.853107Z"
        },
        "id": "i0pffvM6Z769",
        "outputId": "f36b1959-e27f-4eb0-e341-ede11385b989",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean BLUE: 0.13570392186021452\n"
          ]
        }
      ],
      "source": [
        "print(f'mean BLUE: {np.mean(blue_scores)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:27:10.043091Z",
          "iopub.status.busy": "2021-06-30T08:27:10.042771Z",
          "iopub.status.idle": "2021-06-30T08:27:10.049163Z",
          "shell.execute_reply": "2021-06-30T08:27:10.047028Z",
          "shell.execute_reply.started": "2021-06-30T08:27:10.043062Z"
        },
        "id": "X4stXhOFZ769",
        "outputId": "774e14a8-64b9-4d9b-eb47-442b76fece57",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean NIST: 1.8309096441259844\n"
          ]
        }
      ],
      "source": [
        "print(f'mean NIST: {np.mean(Nist_score)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiDdy5MldmqR"
      },
      "source": [
        "## Some of translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-06-30T08:11:40.246734Z",
          "iopub.status.busy": "2021-06-30T08:11:40.246328Z",
          "iopub.status.idle": "2021-06-30T08:11:40.258931Z",
          "shell.execute_reply": "2021-06-30T08:11:40.258142Z",
          "shell.execute_reply.started": "2021-06-30T08:11:40.246703Z"
        },
        "id": "t-b5-lvAZ76-",
        "outputId": "6ea448c9-d709-4700-99d0-136915ce8aa0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: original sentence: hello , do we drive together to Hanover on the twenty-eighth of March ? \n",
            "translation: سلام , ما با هم در حال حرکت هستیم , آیا با هم در روز با هم با هم در هانوور صحبت کنیم ?\n",
            "1: original sentence: it is more comfortable by train . \n",
            "translation: آموزش های بیشتری با قطار است .\n",
            "2: original sentence: do you go by car and I go by train ? \n",
            "translation: می روم با ماشین و با ماشین می روم ?\n",
            "3: original sentence: I would like to go by train . and what would you like ? \n",
            "translation: و من دوست دارم با قطار ? و به شما می روم .\n",
            "4: original sentence: if we take the $I-$C-$E train at six past seven , we will arrive at twenty-five past eight . \n",
            "translation: اگر ما هفت دلار می توانیم هشت دلار در گذشته به قطار سریع السیر <unk> , هفت دلار برسد , هشت دلار خواهیم رسید .\n",
            "5: original sentence: which cafe ? \n",
            "translation: که به آن کافه می آیند ?\n",
            "6: original sentence: the cafe at platform fourteen . \n",
            "translation: چهارده ساله در کافه می باشد .\n",
            "7: original sentence: in any case a cheap hotel . \n",
            "translation: هر یک هتل ارزان .\n",
            "8: original sentence: what did you say , please ? \n",
            "translation: لطفا , چه کاری را انجام دهید ?\n",
            "9: original sentence: and how much is a single room ? \n",
            "translation: و یک اتاق چه اندازه ای است ?\n",
            "10: original sentence: we can take a taxi from the station to the hotel . \n",
            "translation: میتوانیم از ایستگاه تاکسی بگیریم .\n",
            "11: original sentence: at which hotel do you want to reserve a room now and how much is a single room ? \n",
            "translation: و حالا چه مقدار ذخیره ی یک اتاق هتل می خواهید چه مقدار ذخیره ی رزرو کنید ?\n",
            "12: original sentence: okay , should we drive back on Friday evening ? \n",
            "translation: باشه , باید جمعه عصر رانندگی کنیم ?\n",
            "13: original sentence: I think we rather drive back at thirty-three past nine then we will arrive at Hamburg at fifty-two past ten . \n",
            "translation: فکر میکنم ما در نه گذشته به ده و سپس به هامبورگ خواهیم رسید .\n",
            "14: original sentence: fine and don't forget your swimming stuff , maybe we can go swimming together . \n",
            "translation: خوب می توانیم فراموش کنیم که شنا کنیم و <unk> را با هم <unk> و <unk> را با هم <unk> .\n",
            "15: original sentence: yes . when and where do we want to meet ? \n",
            "translation: و کجا می خواهیم ملاقات کنیم ? بله و کجا ?\n",
            "16: original sentence: I prefer the plane . \n",
            "translation: من هواپیما را ترجیح میدهم .\n",
            "17: original sentence: a good idea . then we will meet at the airport tomorrow . \n",
            "translation: پس فردا , ما در فرودگاه ملاقات میکنیم .\n",
            "18: original sentence: no idea . we will see . it does not matter . \n",
            "translation: نه . ما این را می بینیم . نه . نه . نه .\n",
            "19: original sentence: good . let us meet at nine o'clock . hopefully the plane won't be hijacked tomorrow . \n",
            "translation: فردا ساعت نه , هواپیما , اجازه دهید که هواپیما را ملاقات کنیم .\n",
            "20: original sentence: I have already booked two rooms at the Gr\"unschnabel . \n",
            "translation: دو اتاق های <unk> قبلا رزرو شده اند .\n",
            "21: original sentence: what did you say ? \n",
            "translation: چه چیزی ? شما گفتید ?\n",
            "22: original sentence: yes . we have two rooms at the Gr\"unschnabel . I will reserve a taxi right now . \n",
            "translation: بله , من الان دو اتاق رزرو میکنم . درست یک تاکسی در حال حاضر در یک تاکسی هم یک تاکسی .\n",
            "23: original sentence: what is planned for the evening ? \n",
            "translation: عصر چه برنامه ￭-￭ نظر برنامه ￭-￭ ریزی شده است ?\n",
            "24: original sentence: a good idea . I have heard , Phantom of the opera is supposed to be played . \n",
            "translation: این موضوع یک اپرا , خوب است که فکر می کردم که قرار بود شنیده شود .\n",
            "25: original sentence: fine . I think we have arranged everything . then we will meet tomorrow . \n",
            "translation: خوب است , من فکر میکنم فردا همدیگر را ملاقات کنیم .\n",
            "26: original sentence: hello . we have to talk about our trip to Hanover . \n",
            "translation: سلام . ما باید درباره سفر به هانوور صحبت کنیم .\n",
            "27: original sentence: right . we will be at the Expo two thousand in Hanover on the fourth and fifth of September . \n",
            "translation: درست در پنجم سپتامبر ما در ساعت دو هزار و چهارم در ساعت دو و پنجم سپتامبر برگذار خواهیم شد .\n",
            "28: original sentence: I have already booked a flight . \n",
            "translation: من قبلا یک پرواز دارم .\n",
            "29: original sentence: we will set off at a quarter past eight and arrive at Hanover at twelve o'clock . \n",
            "translation: ما در ساعت هشت و دوازده و دوازده یک چهارم به هانوور خواهیم رسید .\n"
          ]
        }
      ],
      "source": [
        "for i in range(30):\n",
        "    sentence = test_sentences[i]\n",
        "    translation = \" \".join(test_results[i])\n",
        "    print(f'{i}: original sentence: {sentence} \\ntranslation: {translation}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbU-wRmLZ77A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DL_HW4_part2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1931afe8a8d7b0a461075628450608118a3125decaa10fc64be4d4dff92469b3"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}